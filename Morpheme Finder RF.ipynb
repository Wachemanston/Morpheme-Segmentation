{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Morpheme Finder with Random Forest\n",
    "We will do morpheme segmentation with random forest in this notebook.\n",
    "## Import & Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from requests import request, ConnectionError\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# env variables\n",
    "try:\n",
    "    with open('.env.json') as f:\n",
    "        ENV_VARIABLES = loads(f.read())\n",
    "        f.close()\n",
    "except FileNotFoundError:\n",
    "    ENV_VARIABLES = {'DATA_DIR': 'C:\\\\'}\n",
    "DATA_DIR = ENV_VARIABLES['DATA_DIR']\n",
    "FTP_DIR = 'http://m106.nthu.edu.tw/~s106062341/morpheme_finder_data/'\n",
    "\n",
    "# file accessor\n",
    "def get_file(filename: str, callback: classmethod) -> bool:\n",
    "    try:\n",
    "        with open(f'{DATA_DIR}{filename}', 'r') as f:\n",
    "            callback(f.read())\n",
    "            f.close()\n",
    "            return True\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            res = request('GET', f'{FTP_DIR}{filename}')\n",
    "            res.encoding = 'Big5'\n",
    "            callback(res.text)\n",
    "            return True\n",
    "        except ConnectionError:\n",
    "            print('HTTP connection failed')\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f'Load failed: {e}')\n",
    "            return False\n",
    "\n",
    "class Word:\n",
    "    def __init__(self, text, origin_affix_list, affix_list=None) -> None:\n",
    "        self.text = text\n",
    "        self.origin_affix_list = origin_affix_list\n",
    "        self.affix_list = affix_list if affix_list else origin_affix_list\n",
    "        self.label = None\n",
    "        self.create_label()\n",
    "        \n",
    "    def create_label(self) -> None:\n",
    "        text = self.text\n",
    "        label = [0] * len(text)\n",
    "        pos = 0\n",
    "        for affix in self.affix_list[1:]:\n",
    "            prev_pos = text.find(affix, pos)\n",
    "            label[prev_pos] = 1\n",
    "            pos = prev_pos + len(affix)\n",
    "        self.label = label\n",
    "        \n",
    "    def get_breakpoints_index(self) -> list:\n",
    "        return [position for position, label in enumerate(self.label) if label]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data & Create its Label\n",
    "1. CELEX.word.and.root.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "bad_celex = []\n",
    "\n",
    "def celex_word_and_root_callback(content: str) -> any:\n",
    "    for line in tqdm(content.split('\\r\\n')):\n",
    "        word, *origin_affix_list = line.split(' ')\n",
    "        if word == ''.join(origin_affix_list):\n",
    "            word_dict[word] = Word(word, origin_affix_list)\n",
    "        else:\n",
    "            bad_celex.append(line)\n",
    "if get_file('CELEX.word.and.root.txt', celex_word_and_root_callback):\n",
    "    print(f'Load CELEX.word.and.root.txt done [{len(word_dict.keys())} / {len(bad_celex)}]')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Feature for Each Character\n",
    "Features include:\n",
    "1. character itself\n",
    "2. character position\n",
    "3. is character a vowel\n",
    "4. distance to the previous breakpoint\n",
    "5. distance to the next breakpoint\n",
    "6. how many breakpoints in preceding substring\n",
    "7. how many breakpoints in succeeding substring\n",
    "8. word length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vowels = {'a', 'e', 'i', 'o', 'u'}\n",
    "def get_ascii(char: str) -> float:\n",
    "    return (ord(char.lower()) - 110) / 26\n",
    "\n",
    "def create_char_feature(word: Word) -> list:\n",
    "    MAX = 100\n",
    "    text = word.text\n",
    "    bps = word.get_breakpoints_index() + [MAX]\n",
    "    features = []\n",
    "    for idx, character in enumerate(text):\n",
    "        dis2prev_bp = -1\n",
    "        dis2next_bp = -1\n",
    "        prec_bp_count = 0\n",
    "        succ_bp_count = 0\n",
    "        for i, bp in enumerate(bps):\n",
    "            if idx < bp:\n",
    "                if i > 0:\n",
    "                    dis2prev_bp = (idx - bps[i-1])\n",
    "                if bp < MAX:\n",
    "                    dis2next_bp = bp - idx\n",
    "                prec_bp_count = i\n",
    "                succ_bp_count = len(bps) - 1 - i\n",
    "                break\n",
    "            if idx == bp:\n",
    "                dis2prev_bp = 0\n",
    "                dis2next_bp = 0\n",
    "                prec_bp_count = i + 1\n",
    "                succ_bp_count = len(bps) - 2 - i\n",
    "                break\n",
    "        features.append([\n",
    "            1,\n",
    "            get_ascii(character),\n",
    "            (idx * 2 / (len(text) - 1)) - 1,\n",
    "            int(character in vowels),\n",
    "            dis2prev_bp,\n",
    "            dis2next_bp,\n",
    "            prec_bp_count,\n",
    "            succ_bp_count,\n",
    "            len(text)\n",
    "        ])\n",
    "        # features.append([\n",
    "        #     'bias',\n",
    "        #     f'char={character}',\n",
    "        #     f'vowel={character in vowels}',\n",
    "        #     f'dis2prev_and_next_bp={dis2prev_bp}:{dis2next_bp}',\n",
    "        #     f'prec_and_succ_bp_count={prec_bp_count}:{succ_bp_count}',\n",
    "        # ])\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Train & Test Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_features = [create_char_feature(w) for w in word_dict.values()]\n",
    "data_label = [w.label for w in word_dict.values()]\n",
    "train_X, test_X, train_y, test_y = train_test_split(data_features, data_label, test_size=0.2)\n",
    "train_X = [ee for e in train_X for ee in e]\n",
    "test_X = [ee for e in test_X for ee in e]\n",
    "train_y = [ee for e in train_y for ee in e]\n",
    "test_y = [ee for e in test_y for ee in e]\n",
    "\n",
    "# train_X = [ee for e in train_y for ee in e]\n",
    "# train_y = [ee for e in test_y for ee in e]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start Train & Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forest = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "forest_fit = forest.fit(train_X, train_y)\n",
    "\n",
    "test_y_predicted = forest.predict(test_X)\n",
    "\n",
    "accuracy = accuracy_score(test_y, test_y_predicted)\n",
    "scores = cross_val_score(forest, train_X + test_X, train_y + test_y, cv=5)\n",
    "precision, recall, fbeta_score, _ = precision_recall_fscore_support(test_y, test_y_predicted, average='weighted', zero_division=0)\n",
    "print(accuracy, scores, precision, recall, fbeta_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def decompose(text: str, idx: int, prec_bp_count: int, succ_bp_count: int) -> tuple:\n",
    "    max_accuracy, max_accuracy_idx = 0, 0\n",
    "    if len(text) == 1 or prec_bp_count > 10 or succ_bp_count > 10:\n",
    "        if prec_bp_count > 10:\n",
    "            print(f'prec ct = {prec_bp_count}')\n",
    "        if succ_bp_count > 10:\n",
    "            print(f'succ ct = {succ_bp_count}')\n",
    "        return text,\n",
    "    for i in range(0, len(text)):\n",
    "        t_y = [0] * len(text)\n",
    "        t_y[i] = 1\n",
    "        p_y = []\n",
    "        for j, char in zip(range(0, len(text)), text):\n",
    "            dis2prev_bp = j if (j < i) else j - i\n",
    "            dis2next_bp = i - j if (j < i) else (len(text) - j)\n",
    "            prec_bp_ct = prec_bp_count + (0 if j < i else 1)\n",
    "            succ_bp_ct = succ_bp_count + (1 if j < i else 0)\n",
    "#             pd = [1, get_ascii(char), idx + j, int(char in vowels), dis2prev_bp, dis2next_bp, prec_bp_ct, succ_bp_ct, len(text)]\n",
    "            pd = [1, get_ascii(char), (idx + j) / len(text), int(char in vowels)]\n",
    "#             print(pd)\n",
    "            p_y.append(forest.predict([pd]))\n",
    "        acc = accuracy_score(t_y, p_y)\n",
    "        print(text, t_y, [list(v) for v in p_y], acc)\n",
    "        if max_accuracy < acc:\n",
    "            max_accuracy = acc\n",
    "            max_accuracy_idx = i\n",
    "    prec = decompose(text[:max_accuracy_idx], 0, prec_bp_count, succ_bp_count+1) if max_accuracy_idx > 0 else None\n",
    "    succ = decompose(text[max_accuracy_idx+1:], idx+max_accuracy_idx, prec_bp_count+1, succ_bp_count) if max_accuracy_idx < (len(text) - 1) else None\n",
    "    decomposition = (prec, succ)\n",
    "    print(decomposition, max_accuracy)\n",
    "    if prec and succ:\n",
    "        return decomposition\n",
    "    elif prec:\n",
    "        return (prec, text[max_accuracy_idx])\n",
    "    elif succ:\n",
    "        return (text[max_accuracy_idx], succ)\n",
    "    else:\n",
    "        return text,\n",
    "        \n",
    "# input_text = input()\n",
    "input_features = decompose('section', 0, 0, 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}