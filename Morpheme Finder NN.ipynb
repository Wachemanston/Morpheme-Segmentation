{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morpheme Finder with Neural Network\n",
    "We will do morpheme segmentation with neural network in this notebook.\n",
    "## Import & Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from math import ceil\n",
    "from random import sample\n",
    "from requests import request, ConnectionError\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# env variables\n",
    "try:\n",
    "    with open('.env.json') as f:\n",
    "        ENV_VARIABLES = loads(f.read())\n",
    "        f.close()\n",
    "except FileNotFoundError:\n",
    "    ENV_VARIABLES = {'DATA_DIR': 'C:\\\\'}\n",
    "DATA_DIR = ENV_VARIABLES['DATA_DIR']\n",
    "FTP_DIR = 'http://m106.nthu.edu.tw/~s106062341/morpheme_finder_data/'\n",
    "\n",
    "# file accessor\n",
    "def get_file(filename: str, callback: classmethod) -> bool:\n",
    "    try:\n",
    "        with open(f'{DATA_DIR}{filename}', 'r') as f:\n",
    "            callback(f.read())\n",
    "            f.close()\n",
    "            return True\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            res = request('GET', f'{FTP_DIR}{filename}')\n",
    "            res.encoding = 'Big5'\n",
    "            callback(res.text)\n",
    "            return True\n",
    "        except ConnectionError:\n",
    "            print('HTTP connection failed')\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f'Load failed: {e}')\n",
    "            return False\n",
    "\n",
    "class Word:\n",
    "    def __init__(self, text, origin_affix_list, affix_list=None) -> None:\n",
    "        self.text = text\n",
    "        self.origin_affix_list = origin_affix_list\n",
    "        self.affix_list = affix_list if affix_list else origin_affix_list\n",
    "        self.label = None\n",
    "        self.create_label()\n",
    "        \n",
    "    def create_label(self) -> None:\n",
    "        text = self.text\n",
    "        label = [0] * len(text)\n",
    "        pos = 0\n",
    "        for affix in self.affix_list[1:]:\n",
    "            prev_pos = text.find(affix, pos)\n",
    "            label[prev_pos] = 1\n",
    "            pos = prev_pos + len(affix)\n",
    "        self.label = label\n",
    "        \n",
    "    def get_breakpoints_index(self) -> list:\n",
    "        return [position for position, label in enumerate(self.label) if label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Create its Label\n",
    "1. CELEX.word.and.root.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22bd75fbfb34432a7d2f097234818a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20066.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load CELEX.word.and.root.txt done [11770 / 8296]\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "bad_celex = []\n",
    "\n",
    "def celex_word_and_root_callback(content: str) -> any:\n",
    "    for line in tqdm(content.split('\\r\\n')):\n",
    "        word, *origin_affix_list = line.split(' ')\n",
    "        if word == ''.join(origin_affix_list):\n",
    "            word_dict[word] = Word(word, origin_affix_list)\n",
    "        else:\n",
    "            bad_celex.append(line)\n",
    "if get_file('CELEX.word.and.root.txt', celex_word_and_root_callback):\n",
    "    print(f'Load CELEX.word.and.root.txt done [{len(word_dict.keys())} / {len(bad_celex)}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature for Each Character\n",
    "Features include:\n",
    "1. character itself\n",
    "2. character position\n",
    "3. is character a vowel\n",
    "4. distance to the previous breakpoint\n",
    "5. distance to the next breakpoint\n",
    "6. how many breakpoints in preceding substring\n",
    "7. how many breakpoints in succeeding substring\n",
    "8. word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vowels = {'a', 'e', 'i', 'o', 'u'}\n",
    "def get_ascii(char: str) -> float:\n",
    "    return (ord(char.lower()) - 110) / 26\n",
    "\n",
    "def create_char_feature(word: Word) -> list:\n",
    "    MAX = 100\n",
    "    text = word.text\n",
    "    bps = word.get_breakpoints_index() + [MAX]\n",
    "    features = []\n",
    "    for idx, character in enumerate(text):\n",
    "        dis2prev_bp = -1\n",
    "        dis2next_bp = -1\n",
    "        prec_bp_count = 0\n",
    "        succ_bp_count = 0\n",
    "        for i, bp in enumerate(bps):\n",
    "            if idx < bp:\n",
    "                if i > 0:\n",
    "                    dis2prev_bp = (idx - bps[i-1])\n",
    "                if bp < MAX:\n",
    "                    dis2next_bp = bp - idx\n",
    "                prec_bp_count = i\n",
    "                succ_bp_count = len(bps) - 1 - i\n",
    "                break\n",
    "            if idx == bp:\n",
    "                dis2prev_bp = 0\n",
    "                dis2next_bp = 0\n",
    "                prec_bp_count = i + 1\n",
    "                succ_bp_count = len(bps) - 2 - i\n",
    "                break\n",
    "        features.append([\n",
    "            # 1,\n",
    "            get_ascii(character),\n",
    "#             (idx * 2 / (len(text) - 1)) - 1,\n",
    "            int(character in vowels),\n",
    "            dis2prev_bp,\n",
    "            dis2next_bp,\n",
    "            # prec_bp_count,\n",
    "            # succ_bp_count,\n",
    "#             len(text)\n",
    "        ])\n",
    "        # features.append([\n",
    "        #     'bias',\n",
    "        #     f'char={character}',\n",
    "        #     f'vowel={character in vowels}',\n",
    "        #     f'dis2prev_and_next_bp={dis2prev_bp}:{dis2next_bp}',\n",
    "        #     f'prec_and_succ_bp_count={prec_bp_count}:{succ_bp_count}',\n",
    "        # ])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225953 225953\n",
      "132773\n",
      "66251 112976\n"
     ]
    }
   ],
   "source": [
    "data_features = [item for w in word_dict.values() for item in create_char_feature(w)]\n",
    "data_label = [label for w in word_dict.values() for label in w.label]\n",
    "\n",
    "label_count = len(data_label)\n",
    "pos_label = list(filter(lambda x: x[1], map(lambda t: (t[0], t[1]), enumerate(data_label))))\n",
    "pos_label_count = len(pos_label)\n",
    "neg_label_count = label_count - pos_label_count\n",
    "if pos_label_count < neg_label_count:\n",
    "    ratio = 1.5\n",
    "    diff = ceil((neg_label_count - pos_label_count) * ratio)\n",
    "    mod = diff % pos_label_count\n",
    "    quotient = int((diff - mod) / pos_label_count)\n",
    "    for _ in range(quotient):\n",
    "        data_label += [1] * pos_label_count\n",
    "        data_features += [data_features[i] for (i, _) in pos_label]\n",
    "    data_label += [1] * mod\n",
    "    data_features += [data_features[i] for (i, _) in sample(pos_label, mod)]\n",
    "else:\n",
    "    print('rev')\n",
    "print(len(data_features), len(data_label))\n",
    "print(len(list(filter(lambda x: x, data_label))))\n",
    "    \n",
    "train_X, test_X, train_y, test_y = train_test_split(data_features, data_label, test_size=0.5)\n",
    "print(len(list(filter(lambda x: x, train_y))), len(train_y))\n",
    "# def k_fold_cv(k):\n",
    "#     p_x = [], n_x = []\n",
    "#     for y, x in zip(data_label, data_features):\n",
    "#         if y:\n",
    "#             p_x.append(x)\n",
    "#         else:\n",
    "#             n_x.append(x)\n",
    "#     p_ratio = len(n_x) / (len(n_x) + len(p_x))\n",
    "#     n_ratio = len(p_x) / (len(n_x) + len(p_x))\n",
    "#     p_sample_range = set(range(len(p_x)))\n",
    "#     n_sample_range = set(range(len(n_x)))\n",
    "#     sample_set_size = ceil((len(p_x) + len(n_x)) / k)\n",
    "#     p_test_sample = set(sample(p_sample_range, ceil(p_ratio * sample_set_size)))\n",
    "#     n_test_sample = set(sample(n_sample_range, ceil(n_ratio * sample_set_size)))\n",
    "#     test_X = [p_x[i] for i in p_test_sample] + [n_x[i] for i in n_test_sample]\n",
    "#     test_y = [1] * len(p_test_sample) + [0] * len(n_test_sample)\n",
    "#     p_sample_range.difference_update(p_test_sample)\n",
    "#     n_sample_range.difference_update(n_test_sample)\n",
    "#     train_X = [p_x[i] for i in p_sample_range] + [n_x[i] for i in n_sample_range]\n",
    "#     train_y = [1] * len(p_sample_range) + [0] * len(n_sample_range)\n",
    "#     print(len(list(filter(lambda x: x, train_y))), len(train_y))\n",
    "#     print(len(list(filter(lambda x: x, test_y))), len(test_y))\n",
    "#     return train_X, train_y, test_X, test_y\n",
    "# train_X, train_y, test_X, test_y = k_fold_cv(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66522 112977\n",
      "1.0 1.0 1.0\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "test_y_predicted = clf.predict(test_X)\n",
    "print(len(list(filter(lambda x: x, test_y_predicted))), len(test_y_predicted))\n",
    "precision, recall, fbeta_score, _ = precision_recall_fscore_support(test_y, test_y_predicted, average='weighted', zero_division=0)\n",
    "print(precision, recall, fbeta_score)\n",
    "\n",
    "scores = cross_val_score(clf, train_X + test_X, train_y + test_y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "international [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "international [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] [0 0 0 0 0 0 0 0 0 0 0 0 0] 0.9230769230769231\n",
      "(None, None) 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "def decompose(text: str, idx: int, prec_bp_count: int, succ_bp_count: int) -> tuple:\n",
    "    max_accuracy, max_accuracy_idx = 0, 0\n",
    "    if len(text) == 1 or prec_bp_count > 10 or succ_bp_count > 10:\n",
    "        if prec_bp_count > 10:\n",
    "            print(f'prec ct = {prec_bp_count}')\n",
    "        if succ_bp_count > 10:\n",
    "            print(f'succ ct = {succ_bp_count}')\n",
    "        return text,\n",
    "    for i in range(0, len(text)):\n",
    "        t_y = [0] * len(text)\n",
    "        t_y[i] = 1\n",
    "        p_y = []\n",
    "        for j, char in zip(range(0, len(text)), text):\n",
    "            dis2prev_bp = j if (j < i) else j - i\n",
    "            dis2next_bp = i - j if (j < i) else (len(text) - j)\n",
    "            prec_bp_ct = prec_bp_count + (0 if j < i else 1)\n",
    "            succ_bp_ct = succ_bp_count + (1 if j < i else 0)\n",
    "#             pd = [get_ascii(char), ((idx + j) * 2 / (len(text) - 1)) - 1, int(char in vowels), dis2prev_bp, dis2next_bp, prec_bp_ct, succ_bp_ct, len(text)]\n",
    "            pd = [get_ascii(char), int(char in vowels), dis2prev_bp, dis2next_bp]\n",
    "#             pd = [1, get_ascii(char), (idx + j) / len(text), int(char in vowels)]\n",
    "#             print(pd)\n",
    "            p_y.append(pd)\n",
    "#         for y in p_y:\n",
    "#             print(y)\n",
    "        p_y = clf.predict(p_y)\n",
    "        acc = accuracy_score(t_y, p_y)\n",
    "        print(text, t_y, p_y, acc)\n",
    "        if max_accuracy < acc:\n",
    "            max_accuracy = acc\n",
    "            max_accuracy_idx = i\n",
    "    prec = decompose(text[:max_accuracy_idx], 0, prec_bp_count, succ_bp_count+1) if max_accuracy_idx > 0 else None\n",
    "    succ = decompose(text[max_accuracy_idx:], idx+max_accuracy_idx, prec_bp_count+1, succ_bp_count) if max_accuracy_idx > 0 and max_accuracy_idx <= (len(text) - 1) else None\n",
    "    decomposition = (prec, succ)\n",
    "    print(decomposition, max_accuracy)\n",
    "    if prec and succ:\n",
    "        return decomposition\n",
    "    elif prec:\n",
    "        return prec,\n",
    "    elif succ:\n",
    "        return succ,\n",
    "    else:\n",
    "        return text,\n",
    "        \n",
    "# input_text = input()\n",
    "input_features = decompose('international', 0, 0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
